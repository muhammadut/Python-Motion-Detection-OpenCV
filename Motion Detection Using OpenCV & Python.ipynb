{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Detection using OpenCV (A beginner's guide) \n",
    "\n",
    " \n",
    "\n",
    "This notebook is a step-by-step explanation of a simple OpenCV Motion Detection application. \n",
    "\n",
    "This was a  part of a computer vision project I did during my internship at Honda of Canada MFG. \n",
    "\n",
    "I hope you are familiar with some of OpenCV's functions. If not, then don't worry I will try to explain everything in detail as much as possible. \n",
    "\n",
    "\n",
    "## High Level Explanantion:\n",
    "\n",
    "The general idea behind the application is that it takes the first frame from the camera  as a reference. \n",
    "\n",
    "This reference frame is a static background against which we are trying to detect motion.  \n",
    "\n",
    "The program will run & detect changes between the first reference frame and the frames that follow. \n",
    "\n",
    "Basically, \n",
    "\n",
    "We are going to compare pixel difference \"delta\" between the first static reference frame & other frames.\n",
    "\n",
    "If the difference \"delta\" is more than the threshold (that we have defined) it will trigger to draw a box around the areas where the pixel differnence is observed.\n",
    "\n",
    "\n",
    "We will also record times at which the object enters & exits the frame.we will store these values in a Pandas dataframe. And finally, We will visualize these times using Bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Getting started with OpenCV\n",
    "\n",
    "### Webcam basics: \n",
    "\n",
    "__1. Turn on your webcam using OpenCV and Python __\n",
    "\n",
    "To do this we will use the __.VideoCapture()__ method from OpenCV. The input argument is the camera(s) connected to your computer. In our case it is only 1 camera so we will enter 0. If you had 2 or 3 cameras you'd put in 1 or 2.\n",
    "\n",
    "__2. Release the camera__ \n",
    "\n",
    "If you run the first line of code: __video_object = cv2.VideoCapture__(0) below you would notice the light turns on your camera. That is because we have intialized the camera. The light would not turn off. To turn off the camera we would like to use the: \n",
    "__.release() method__. This will turn off the camera or will _release_ it.\n",
    "\n",
    "__3. Holding the camera for a certain amount of time__\n",
    "\n",
    "Now you'd ask that what if I want to run the camera for a certain amount of time? For that we will import _time_ and use the __.sleep() method__ on \"time\" object to tell Python for how long we'd like to hold the camera before it is realeased.\n",
    "\n",
    "So far so good...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # import OpenCV\n",
    "import time # import time to hold the camera for a certain amount of time\n",
    "\n",
    "video_object = cv2.VideoCapture(0) # intialize the camera by creating a video_object\n",
    "\n",
    "time.sleep(3) # hold the camera for 3 seconds \n",
    "\n",
    "video_object.release( ) # release the camera "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. Displaying the first frame__: One \"frame\" at a time\n",
    "\n",
    "The way OpenCV works is that it recursively shows each frame. The first frame after, then the next, then the next, so on and so forth. \n",
    "Let's just start by displaying the first frame. And I know you're already thinking in terms of writing a loop to show all the frames as video feed...but just hang on yet! \n",
    "\n",
    "To display anything on your computer screen you would like to use yet another method __.read()__ on your video_object\n",
    "This outputs two 2 things: \n",
    "1. __A Boolean__, True indicates the camera is turned on & works (can be later used to check if the feed is running etc.) \n",
    "2. __A numpy array__ which is basically a representation of the frame as an array of pixel values. This numpy array is very useful as we can perform operations on it directly. \n",
    "\n",
    "Finally to show what we captured, we will use the __.imshow()__ method. This takes 2 arguments: \n",
    "1. Name of the window that will pop up, & \n",
    "2. The frame that you captured using the __.read()__ method\n",
    "\n",
    "\n",
    "__5. Closing the display window:__\n",
    "\n",
    "We can't just let the window hang there & freeze, we would like to close the window or allow the user to press any key & stop the script. This is very important because if you don't include __.waitKey()__ method along with __.destroyAllWindows()__ then your python kernel might crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bool'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import cv2 # import OpenCV\n",
    "import time # import time to hold the camera for a certain amount of time\n",
    "\n",
    "video_object = cv2.VideoCapture(0) # intialize the camera by creating a video_object\n",
    "\n",
    "check, frame = video_object.read() # check is a boolean, frame is a numpy array \n",
    "                                   \n",
    "time.sleep(3) # hold the camera for 3 seconds \n",
    "\n",
    "cv2.imshow(\"First Frame Captured\", frame) # a window is created named: 1st arguement, & displays: 2nd argument\n",
    "\n",
    "\n",
    "cv2.waitKey(0) # user presses any key which is represented by 0 as an argument\n",
    "\n",
    "video_object.release() # the camera is released as soon as waitKey is pressed\n",
    "\n",
    "cv2.destroyAllWindows() # All windows are closed\n",
    "\n",
    "\n",
    "print(type(check)) # make sure check is boolean \n",
    "print(type(frame)) # frame is a numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we can turn on the webcam, we understand the basic methods in OpenCV. One important thing we learned here is that Python is processing the webcam feed as single images. What does that mean? \n",
    "    It means that we can apply other methods such as converting the images to gray scale, performing computations etc... this is good. \n",
    "    \n",
    "    \n",
    "## __Video Feed!__\n",
    "\n",
    "Ok so now, we want the camera to run a live feed. The simplest way to do it is put the entire block of code above inside a __While loop__ & set it to __True__\n",
    "\n",
    "However we need to make a few modifications first: \n",
    "\n",
    "We can start off first by removing __time.sleep()__\n",
    "\n",
    "Store __.waitKey()__ in a variable called key\n",
    "\n",
    "If the value of this variable is set to something then we stop the video feed.\n",
    "\n",
    "It will make sense when you see the code. Look at the code below first & then read this description in the next Markdown\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # import OpenCV\n",
    "\n",
    "video_object = cv2.VideoCapture(0) # intialize the camera by creating a video_object\n",
    "\n",
    "while True:\n",
    "    check, frame = video_object.read() # save the frame in a variable called frame\n",
    "    \n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # convert image to gray scale: because it is 1 frame at a time, we can apply color transformation \n",
    "\n",
    "    cv2.imshow(\"Video Feed inside while loop\", gray_frame) # create a window but this time we pass the gray_frame as argument to show    \n",
    "    \n",
    "    key = cv2.waitKey(1000)  # wait 1 second or 1000 ms before jumping back to the start of the loop\n",
    "    \n",
    "    if key == ord('q'):   # if the user pressed 'q' on their keyboard break the loop \n",
    "        break\n",
    "    \n",
    "\n",
    "video_object.release() # as soon as 'q' is pressed webcam is released \n",
    "\n",
    "cv2.destroyAllWindows() # All windows are closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What did we just do?__\n",
    "\n",
    "OK now, so this is how the script works: \n",
    "\n",
    "1. We initialize the camera feed in an object called video_object... Basically turns the camera on\n",
    "\n",
    "2. We say __while True:__  \n",
    "\n",
    "Save the frame\n",
    "\n",
    "Convert it into a gray-scale-image using the __.cvtColor()__ method. \n",
    "\n",
    "Show the image\n",
    "\n",
    "Wait for 1000 milli seconds == 1 second \n",
    "\n",
    "Go back to the start of the loop & start over again \n",
    "\n",
    "3. If the user decided to press 'q' on his keyboard then release the camera and close all the windows \n",
    "\n",
    "Simple! :) \n",
    "\n",
    "Now, you can run the script... but there is  __LAG!!__ \n",
    "\n",
    "I feel ya. Here is a small quiz for you: What value in the script can we change to make the feed more smooth? \n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "You guessed it ! \n",
    "We can lower the waitKey() to a lower number so the while loop runs faster & we get more frames/second\n",
    "\n",
    "Try using the __cv2.waitKey(1)__ & see how it goes. \n",
    "\n",
    "Note: waitKey() only accepts integers! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Motion Detection! \n",
    "\n",
    "## Basic architecture of the application: \n",
    "\n",
    "1. So we would like to achieve motion detection through pixel difference computation.\n",
    "\n",
    "2. We would start off by capturing the first frame in our application as our __static background__ \n",
    "\n",
    "3. We will store this static background (first frame) as a numpy array, which is basically a big matrix\n",
    "\n",
    "4. We will apply matrix subtraction on all the frames that follow. \n",
    "\n",
    "5. If the subtraction is more than a certain value (threshold). We will say motion is detected & draw a box around it. \n",
    "\n",
    "LET'S PROCEED!\n",
    "\n",
    "### Store the first_frame! \n",
    "\n",
    "We would start by storing the first frame. It is a little tricky we will have to use the _continue_ statement\n",
    "\n",
    "So we intialize a variable named __first_frame__  to _None_\n",
    "\n",
    "Inside our _while loop_ we write an __if loop__ \n",
    "\n",
    "On the first iteration: if the value of first_frame is None then take the value from gray_frame & store it in the first_frame variable. \n",
    "\n",
    "Go back to the start of the loop & then run the 2nd iteration.\n",
    "\n",
    "We add a _continue_ because we don't want to go execute the lines below before grabbing the frames that follow. \n",
    "\n",
    "So: 1st iteration the first_frame variable is assinged gray_frame from __.read()__ method which is the first frame\n",
    "\n",
    "_continue_ statement sends us back to the start of the while loop so we can grab the 2nd frame for comparison computation.\n",
    "\n",
    "2nd iteration: the if statement is False because the value of first_frame is __NOT None__ as we assigned it a value in the first iteration \n",
    "\n",
    "The if loop is not executed & we go down doing our computations: \n",
    "\n",
    "## Calculate the delta_frame!: \n",
    "\n",
    "Before we apply any delta computations we would like to do some transformations. These are important to get accurate results & to remove noise. \n",
    "\n",
    "The first transformation we will apply to all the frames is a:\n",
    "\n",
    "__.GaussianBlur()__ which takes in three arguments: \n",
    "\n",
    "1. The frame to apply the transformation on, \n",
    "\n",
    "2. The kernel size (as a tuple of width & height) \n",
    "\n",
    "3. Standard deviation of the blur. You can read more on OpenCV blurs on: https://docs.opencv.org/3.1.0/d4/d13/tutorial_py_filtering.html\n",
    "\n",
    "If you dont wan't to read then we will be using kernel size = (21,21), standard_deviation = 0 which are acceptable numbers for our application. \n",
    "\n",
    "\n",
    "Now, we will finally compute the delta_frame using: \n",
    "\n",
    "__.absdiff() method__ This takes 2 arguments which are the the subtraction matrices you'd like to calculate the difference on\n",
    "\n",
    "Finally, you'd like to show the delta_frame for your own understanding.\n",
    "\n",
    "To do this we will create a new window using __.imshow()__ method.\n",
    "\n",
    "__PLEASE NOTE: DISAPPEAR FROM THE WEBCAM WINDOW BEFORE RUNNING THE SCRIPT & THEN SHOWING UP AS AN OBJECT __ to see the delta_frame properly\n",
    "\n",
    "You will see that in the delta_frame window the background is eliminated & only you appear as a negative.... You get the idea.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "\n",
    "first_frame = None # initalize the first frame to None. This will be our reference static background  \n",
    "\n",
    "video_object = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    check, frame = video_object.read() \n",
    "    \n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # convert frame to grayscale\n",
    "    \n",
    "    gray_frame = cv2.GaussianBlur(gray_frame,(21,21),0) # apply blur to remove noise & smoothing\n",
    "    \n",
    "    if first_frame is None: \n",
    "        first_frame = gray_frame\n",
    "        continue  # grab the first frame on 1st iteration & go back to the start of the loop\n",
    "    \n",
    "    delta_frame = cv2.absdiff(first_frame, gray_frame)  # calculate the difference between the first frame & the frames that follow \n",
    "     \n",
    "        \n",
    "    cv2.imshow(\"delta_frame\", delta_frame)    # show the delta frame feed  \n",
    "    cv2.imshow(\"Video Feed inside while loop\", gray_frame)     # just the normal gray scale & blurred feed\n",
    "    \n",
    "    key = cv2.waitKey(1)  \n",
    "    if key == ord('q'):    \n",
    "        break\n",
    "    \n",
    "\n",
    "video_object.release()  \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calulate the Threshold!\n",
    "\n",
    "Now that we have our delta_frame we would like calculate the threshold.\n",
    "\n",
    "The idea is that delta_frame is a numpy matrix with integers as values acquired from the subtraction of the first_frame with the current frame. \n",
    "\n",
    "The higher these values in the matrix the greater the difference between the first_frame & the current frame...so something changed between the two\n",
    "\n",
    "You can check out the delta_frame matrix by simply typing \n",
    "__print(delta_frame)__ \n",
    "\n",
    "\n",
    "\n",
    "To calculate the threshold we use: \n",
    "\n",
    "__.threshold() method__ the method takes in 4 arguments: \n",
    "\n",
    "1. The threshold matrix in our case delta_frame\n",
    "2. The threshold limit. I set it to be 30. You can adjust it to your liking\n",
    "3. The color to assign to the pixel coordinates that are more than the threshold. I set it to be 255 which converts them to white color\n",
    "4. The threshold method in our case it is __binary threshold__ \n",
    "\n",
    "The __.threshold()__ method returns a tuple with 2 values: \n",
    "\n",
    "We need the 2nd value of this tuple which is the actual frame we access it by indexing it by using [1] \n",
    "\n",
    "\n",
    "In summary: \n",
    "So, if the intensity of the pixel is higher than threshold limit we defined, then the new pixel intensity is set to 255. Otherwise, the pixels are set to 0. \n",
    "You can read on thresholding at: https://docs.opencv.org/2.4/doc/tutorials/imgproc/threshold/threshold.html\n",
    "\n",
    "We can go ahead & use the threshold_frame & draw boxes on it to indicate motion but before we do that we would want to make the white areas smoother. \n",
    "\n",
    "\n",
    "To _smooth_ out the threshold_frame even more we will finally use: \n",
    "\n",
    "__.dilate() method__  which takes 3 arguments: \n",
    "\n",
    "1. The threshold_frame to perform smoothing on.\n",
    "2. The kernel array for custom dilation. I choose None\n",
    "3. The number of iterations to perform the smoothing. The higher the smoother. I choose 2 as our value.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "\n",
    "first_frame = None \n",
    "video_object = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    check, frame = video_object.read() \n",
    "    \n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    gray_frame = cv2.GaussianBlur(gray_frame,(21,21),0) \n",
    "    \n",
    "    if first_frame is None: \n",
    "        first_frame = gray_frame\n",
    "        continue  \n",
    "    \n",
    "    delta_frame = cv2.absdiff(first_frame, gray_frame)\n",
    "    \n",
    "    threshold_frame = cv2.threshold(delta_frame,30,255,cv2.THRESH_BINARY)[1] # extracting the threshold_frame \n",
    "    \n",
    "    threshold_frame = cv2.dilate(threshold_frame, None, iterations = 2) # applying threshold smoothing using dilate\n",
    "        \n",
    "    cv2.imshow(\"threshold frame\", threshold_frame)\n",
    "    \n",
    "    key = cv2.waitKey(1)  \n",
    "    if key == ord('q'):    \n",
    "        break\n",
    "    \n",
    "\n",
    "video_object.release()  \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding & Drawing Contours! \n",
    "\n",
    "To find ALL the contours we use the OpenCV's:\n",
    "\n",
    "__.findContours() method__ the method takes 3 arguments: \n",
    "\n",
    " \n",
    "1. The frame to find contours from. \n",
    "2. The contour retrieval mode. \n",
    "3. The approximation method.\n",
    "\n",
    "It outputs 3 values: \n",
    "\n",
    "1. The modified image\n",
    "2. The contours\n",
    "3. The hierarchy\n",
    "\n",
    "You can read more on: https://docs.opencv.org/3.4.0/d4/d73/tutorial_py_contours_begin.html\n",
    "\n",
    "\n",
    "We are interested in the 2nd output from the findContours() method.\n",
    "\n",
    "The 2nd output is all the contours in the current frame, but we are only interested in the contours that are let's just say for example bigger than X-pixels. \n",
    "\n",
    "So we will have to iterate over the contours & say if the area is bigger than X-pixels then draw them. \n",
    "\n",
    "To do so we will use 2 more functions as follows: \n",
    "\n",
    "\n",
    "__.boundingRect():__ It takes in the contour as input & returns 4 values: \n",
    "\n",
    "x coordinate, y coordinate, width, height of the contour\n",
    "\n",
    "Then we use: \n",
    "\n",
    "__.rectangle()__ method to draw a rectange using these coordinated: \n",
    "\n",
    "The inputs are as follows: \n",
    "\n",
    "1. The original frame. \n",
    "2. x, y the upper left corner of the box \n",
    "3. x+w & y+h as the lower right corner of the box \n",
    "4. The color of the box in the form of a tuple\n",
    "5. width \n",
    "\n",
    "\n",
    "A side note: the larger you set the area to in the if-loop the bigger objects will be detected. You would have to adjust the area of pixels in the contour section to fit your appliation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "\n",
    "first_frame = None \n",
    "video_object = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    check, frame = video_object.read() \n",
    "    \n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    gray_frame = cv2.GaussianBlur(gray_frame,(21,21),0) \n",
    "    \n",
    "    if first_frame is None: \n",
    "        first_frame = gray_frame\n",
    "        continue  \n",
    "    \n",
    "    ## delta_frame calculation & smoothing\n",
    "    \n",
    "    delta_frame = cv2.absdiff(first_frame, gray_frame)\n",
    "    threshold_frame = cv2.threshold(delta_frame,30,255,cv2.THRESH_BINARY)[1]  \n",
    "    threshold_frame = cv2.dilate(threshold_frame, None, iterations = 2) \n",
    "    \n",
    "    \n",
    "    ## Finding & drawing contours \n",
    "    \n",
    "    (_,contours,_) = cv2.findContours(threshold_frame.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) ## finding contours \n",
    "    \n",
    "    for contour in contours:\n",
    "        \n",
    "        if cv2.contourArea(contour) < 1000:\n",
    "            continue\n",
    "        \n",
    "        (x,y,w,h) = cv2.boundingRect(contour)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0,255,0), 3)\n",
    "    \n",
    "    \n",
    "    cv2.imshow(\"Motion Detection\", frame)\n",
    "               \n",
    "    key = cv2.waitKey(1)  \n",
    "    if key == ord('q'):    \n",
    "        break\n",
    "    \n",
    "\n",
    "video_object.release()  \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voila! \n",
    "\n",
    "###  NOTE: Disappear infront of the webcam, then press run, appear infront of the webcam & you will see a green box drawn around you. \n",
    "\n",
    "### That's the motion detection.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Storing time & refining the application\n",
    "\n",
    "\n",
    "We have reached this far... yay!  \n",
    "\n",
    "We would now like to record the times at which the an object enters & exits the frame. \n",
    "\n",
    "To do this we would need to find a place in our script where the state transforms from motion to no-motion. \n",
    "We will let 0 represent no-motion, & 1 represent motion\n",
    "\n",
    "We will set state variable to 0 at the start of the script \n",
    "Then we will go in the contour if-loop, as soon as the contour is bigger than the pixel area we defined, we will change the value of the state variable to 1. \n",
    "\n",
    "We will append the state variable to a list called __state_list__. So this list will have a bunch of 0s & 1s \n",
    "\n",
    "In the state_list we would like to record date-time when there is a change\n",
    "i.e. __we go from 0 to 1 and 1 to 0.__\n",
    "\n",
    "To do this we will write a if-loop with a following conditional: \n",
    "\n",
    "__if state_list's last item is 1 AND state_list's 2nd last item is 0: \n",
    "then record date-time__\n",
    "\n",
    "Similarly,\n",
    "\n",
    "__if state_list's last item is 0 AND state_list's 2nd last item is 1: \n",
    "then record date-time__\n",
    "\n",
    "\n",
    "Now we need to make a few more modifications: \n",
    "\n",
    "If we quit the window without exiting the frame then we will have odd number of date & time in our date_time list. Later we would like to iterate through this list and take every other element and append it to a data frame. With having odd numbers iteration would be difficult. \n",
    "\n",
    "Moreover, we need to intialize the date time list with None & None because you will get errors if you don't. \n",
    "\n",
    "To resolve the odd number of date time we can simply add another conditional that says: \n",
    "\n",
    "If user presses 'q' and state is 1: \n",
    "Append the current datetime to the list. \n",
    "\n",
    "\n",
    "Finally, we will take the date_time list and create a dataframe. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import pandas as pd \n",
    "from datetime import datetime\n",
    "\n",
    "state_list = [None, None] # list to record state change\n",
    "date_time = [] # list to record date-time\n",
    "df = pd.DataFrame(columns = [\"Object_Entered\", \"Object_Exited\"]) # initialize a dataframe to append the date time values in. \n",
    "\n",
    "\n",
    "first_frame = None \n",
    "video_object = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    check, frame = video_object.read() \n",
    "    \n",
    "    \n",
    "    state = 0 # set the state variable to 0 to represent no-motion\n",
    "    \n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.GaussianBlur(gray_frame,(21,21),0) \n",
    "    \n",
    "    \n",
    "    \n",
    "    if first_frame is None: \n",
    "        first_frame = gray_frame\n",
    "        continue  \n",
    "    \n",
    "    ## delta_frame calculation & smoothing\n",
    "    \n",
    "    delta_frame = cv2.absdiff(first_frame, gray_frame)\n",
    "    threshold_frame = cv2.threshold(delta_frame,30,255,cv2.THRESH_BINARY)[1]  \n",
    "    threshold_frame = cv2.dilate(threshold_frame, None, iterations = 2) \n",
    "    \n",
    "    \n",
    "    ## Finding & drawing contours \n",
    "    \n",
    "    (_,contours,_) = cv2.findContours(threshold_frame.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) \n",
    "    \n",
    "    for contour in contours:\n",
    "        \n",
    "        if cv2.contourArea(contour) < 1000:\n",
    "            continue\n",
    "        \n",
    "        state = 1 # set the state variable to 1 to represent motion\n",
    "        \n",
    "        (x,y,w,h) = cv2.boundingRect(contour)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0,255,0), 3)\n",
    "    \n",
    "    \n",
    "    state_list.append(state) # append the state_list with current status\n",
    "    \n",
    "    state_list = state_list[-2:] # keep the last two items to avoid memory problems if the application is run for too long\n",
    "     \n",
    "    if state_list[-1] == 1 and state_list[-2] == 0:\n",
    "        date_time.append(datetime.now())\n",
    "    \n",
    "    if state_list[-1] == 0 and state_list[-2] == 1:\n",
    "        date_time.append(datetime.now())\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    cv2.imshow(\"Motion Detection\", frame)\n",
    "               \n",
    "    key = cv2.waitKey(1)  \n",
    "    if key == ord('q'):\n",
    "        if state == 1:\n",
    "            date_time.append(datetime.now())\n",
    "        break\n",
    "\n",
    "        \n",
    "for i in range(0, len(date_time),2):\n",
    "    df = df.append({\"Object_Entered\": date_time[i], \"Object_Exited\" :date_time[i+1],}, ignore_index = True)\n",
    "\n",
    "df.to_csv(\"motion_detection_times.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "video_object.release()  \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Object_Entered</th>\n",
       "      <th>Object_Exited</th>\n",
       "      <th>Start_string</th>\n",
       "      <th>End_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-08-05 21:57:41.577635</td>\n",
       "      <td>2018-08-05 21:57:42.745421</td>\n",
       "      <td>2018-08-05 21:57:41</td>\n",
       "      <td>2018-08-05 21:57:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-08-05 21:57:44.585196</td>\n",
       "      <td>2018-08-05 21:57:46.681380</td>\n",
       "      <td>2018-08-05 21:57:44</td>\n",
       "      <td>2018-08-05 21:57:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-08-05 21:57:48.025584</td>\n",
       "      <td>2018-08-05 21:57:50.522093</td>\n",
       "      <td>2018-08-05 21:57:48</td>\n",
       "      <td>2018-08-05 21:57:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-08-05 21:57:52.265240</td>\n",
       "      <td>2018-08-05 21:57:57.865667</td>\n",
       "      <td>2018-08-05 21:57:52</td>\n",
       "      <td>2018-08-05 21:57:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-08-05 21:57:57.945614</td>\n",
       "      <td>2018-08-05 21:57:59.914144</td>\n",
       "      <td>2018-08-05 21:57:57</td>\n",
       "      <td>2018-08-05 21:57:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Object_Entered              Object_Exited         Start_string  \\\n",
       "0 2018-08-05 21:57:41.577635 2018-08-05 21:57:42.745421  2018-08-05 21:57:41   \n",
       "1 2018-08-05 21:57:44.585196 2018-08-05 21:57:46.681380  2018-08-05 21:57:44   \n",
       "2 2018-08-05 21:57:48.025584 2018-08-05 21:57:50.522093  2018-08-05 21:57:48   \n",
       "3 2018-08-05 21:57:52.265240 2018-08-05 21:57:57.865667  2018-08-05 21:57:52   \n",
       "4 2018-08-05 21:57:57.945614 2018-08-05 21:57:59.914144  2018-08-05 21:57:57   \n",
       "\n",
       "            End_string  \n",
       "0  2018-08-05 21:57:42  \n",
       "1  2018-08-05 21:57:46  \n",
       "2  2018-08-05 21:57:50  \n",
       "3  2018-08-05 21:57:57  \n",
       "4  2018-08-05 21:57:59  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Start_string\"]=df[\"Object_Entered\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "df[\"End_string\"]=df[\"Object_Exited\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations:\n",
    "\n",
    "We will now visualize the datetimes in our dataframe. For this we will use Bokeh interactive visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"df02bb2c-cc9e-4f0a-91f1-7631b58f8394\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"df02bb2c-cc9e-4f0a-91f1-7631b58f8394\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"df02bb2c-cc9e-4f0a-91f1-7631b58f8394\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'df02bb2c-cc9e-4f0a-91f1-7631b58f8394' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.13.0.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"df02bb2c-cc9e-4f0a-91f1-7631b58f8394\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"df02bb2c-cc9e-4f0a-91f1-7631b58f8394\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"df02bb2c-cc9e-4f0a-91f1-7631b58f8394\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'df02bb2c-cc9e-4f0a-91f1-7631b58f8394' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.13.0.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"df02bb2c-cc9e-4f0a-91f1-7631b58f8394\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"2fd45c61-079b-4c7c-be32-1fe268b7a6ef\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"2f17e818-f8c5-4ea1-984a-bfbcfeb9fb96\":{\"roots\":{\"references\":[{\"attributes\":{\"months\":[0,6]},\"id\":\"61f3d5f2-3e9f-43c5-8a55-729afa31df0d\",\"type\":\"MonthsTicker\"},{\"attributes\":{},\"id\":\"e85b75ce-ae6a-4a30-bf9e-92c6454a239a\",\"type\":\"YearsTicker\"},{\"attributes\":{\"base\":24,\"mantissas\":[1,2,4,6,8,12],\"max_interval\":43200000.0,\"min_interval\":3600000.0,\"num_minor_ticks\":0},\"id\":\"fa427535-6aa3-4e5b-ad37-bc4126a94dc4\",\"type\":\"AdaptiveTicker\"},{\"attributes\":{},\"id\":\"7f35f7a1-3f21-4bd1-abe0-584dc1df639f\",\"type\":\"Selection\"},{\"attributes\":{\"callback\":null,\"renderers\":\"auto\",\"tooltips\":[[\"Enter\",\"(@Start_string)\"],[\"Exit\",\"(@End_string)\"]]},\"id\":\"3ea7a304-2cf5-4edd-b3b7-68338083b753\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"b0f80a6b-85c0-480b-a562-85983c295514\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"days\":[1,4,7,10,13,16,19,22,25,28]},\"id\":\"a89741a3-f487-48bf-8fcc-45835fc683f7\",\"type\":\"DaysTicker\"},{\"attributes\":{\"months\":[0,1,2,3,4,5,6,7,8,9,10,11]},\"id\":\"d287a14b-55ed-4f53-b100-0d9ee9341369\",\"type\":\"MonthsTicker\"},{\"attributes\":{\"days\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]},\"id\":\"34ee125a-76c2-400e-a2fe-ee82527e3ebf\",\"type\":\"DaysTicker\"},{\"attributes\":{\"base\":60,\"mantissas\":[1,2,5,10,15,20,30],\"max_interval\":1800000.0,\"min_interval\":1000.0,\"num_minor_ticks\":0},\"id\":\"05d106bb-3298-4339-be79-345751c81336\",\"type\":\"AdaptiveTicker\"},{\"attributes\":{\"source\":{\"id\":\"aa7bd4d1-e7ce-40af-9ec9-8ffae15cad6a\",\"type\":\"ColumnDataSource\"}},\"id\":\"3f6fa948-1214-480f-a046-d23be89988a3\",\"type\":\"CDSView\"},{\"attributes\":{\"callback\":null},\"id\":\"848f9ac9-2b1e-493d-8009-6cb7be269db5\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"67ec2ebd-e7d2-426c-9f80-a384010e6684\",\"type\":\"DatetimeTickFormatter\"},{\"attributes\":{\"desired_num_ticks\":1},\"id\":\"ebca3c5f-c409-42c3-bac0-cad93d1a38a8\",\"type\":\"BasicTicker\"},{\"attributes\":{\"callback\":null,\"data\":{\"End_string\":[\"2018-08-05 21:57:42\",\"2018-08-05 21:57:46\",\"2018-08-05 21:57:50\",\"2018-08-05 21:57:57\",\"2018-08-05 21:57:59\"],\"Object_Entered\":{\"__ndarray__\":\"KZokicFQdkIjk+CJwVB2QliZt4rBUHZC15PAi8FQdkLTmSONwVB2Qg==\",\"dtype\":\"float64\",\"shape\":[5]},\"Object_Exited\":{\"__ndarray__\":\"vJZticFQdkIUlmOKwVB2Qn2hU4vBUHZCrJoejcFQdkJOop6NwVB2Qg==\",\"dtype\":\"float64\",\"shape\":[5]},\"Start_string\":[\"2018-08-05 21:57:41\",\"2018-08-05 21:57:44\",\"2018-08-05 21:57:48\",\"2018-08-05 21:57:52\",\"2018-08-05 21:57:57\"],\"index\":[0,1,2,3,4]},\"selected\":{\"id\":\"7f35f7a1-3f21-4bd1-abe0-584dc1df639f\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"b0f80a6b-85c0-480b-a562-85983c295514\",\"type\":\"UnionRenderers\"}},\"id\":\"aa7bd4d1-e7ce-40af-9ec9-8ffae15cad6a\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"formatter\":{\"id\":\"9dd29f06-050b-473e-92f3-8077d8b11cd4\",\"type\":\"BasicTickFormatter\"},\"minor_tick_line_color\":{\"value\":null},\"plot\":{\"id\":\"f7eb941a-fec3-40a6-a79a-923a715f8693\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"ebca3c5f-c409-42c3-bac0-cad93d1a38a8\",\"type\":\"BasicTicker\"}},\"id\":\"74a0e275-362e-4a0a-a3de-ac386fe12352\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"ff635ee5-3251-40c4-aca8-630c5c3c22bc\",\"type\":\"LinearScale\"},{\"attributes\":{\"days\":[1,8,15,22]},\"id\":\"d82e2ef6-6b8c-4e5b-bc92-0f8b3c4dfcf6\",\"type\":\"DaysTicker\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"3ea7a304-2cf5-4edd-b3b7-68338083b753\",\"type\":\"HoverTool\"},{\"id\":\"3ea7a304-2cf5-4edd-b3b7-68338083b753\",\"type\":\"HoverTool\"}]},\"id\":\"4d11e0c7-295a-4cf8-9b3f-a7d7d6c13651\",\"type\":\"Toolbar\"},{\"attributes\":{\"plot\":{\"id\":\"f7eb941a-fec3-40a6-a79a-923a715f8693\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"7bb54e78-6a53-43ff-a286-44643943044d\",\"type\":\"DatetimeTicker\"}},\"id\":\"bbbc79c3-8f3e-4c8f-ac60-00b60d9b5482\",\"type\":\"Grid\"},{\"attributes\":{\"bottom\":{\"value\":0},\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"left\":{\"field\":\"Object_Entered\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"right\":{\"field\":\"Object_Exited\"},\"top\":{\"value\":1}},\"id\":\"2a7b5d60-3a3f-41eb-b755-d1960a1b7887\",\"type\":\"Quad\"},{\"attributes\":{\"plot\":null,\"text\":\"Motion Detection Graph\"},\"id\":\"bb08fb00-767b-449b-bdd8-90b02811ccb7\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"4484bf60-654f-45e9-80cf-b47c05da6073\",\"type\":\"LinearScale\"},{\"attributes\":{\"data_source\":{\"id\":\"aa7bd4d1-e7ce-40af-9ec9-8ffae15cad6a\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"4d49c838-1497-41d8-a5f0-9fd214d0486b\",\"type\":\"Quad\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"2a7b5d60-3a3f-41eb-b755-d1960a1b7887\",\"type\":\"Quad\"},\"selection_glyph\":null,\"view\":{\"id\":\"3f6fa948-1214-480f-a046-d23be89988a3\",\"type\":\"CDSView\"}},\"id\":\"59e4054f-64ea-4f15-a66d-29e344be189a\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null},\"id\":\"0ee1789b-6079-4bb5-94a1-b65b08c3e26f\",\"type\":\"DataRange1d\"},{\"attributes\":{\"below\":[{\"id\":\"97a165ee-0959-41ce-8803-d300b87d829b\",\"type\":\"DatetimeAxis\"}],\"left\":[{\"id\":\"74a0e275-362e-4a0a-a3de-ac386fe12352\",\"type\":\"LinearAxis\"}],\"plot_height\":200,\"renderers\":[{\"id\":\"97a165ee-0959-41ce-8803-d300b87d829b\",\"type\":\"DatetimeAxis\"},{\"id\":\"bbbc79c3-8f3e-4c8f-ac60-00b60d9b5482\",\"type\":\"Grid\"},{\"id\":\"74a0e275-362e-4a0a-a3de-ac386fe12352\",\"type\":\"LinearAxis\"},{\"id\":\"a085a0bd-a793-4988-890e-9a8dd0a5b725\",\"type\":\"Grid\"},{\"id\":\"59e4054f-64ea-4f15-a66d-29e344be189a\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"bb08fb00-767b-449b-bdd8-90b02811ccb7\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"4d11e0c7-295a-4cf8-9b3f-a7d7d6c13651\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"0ee1789b-6079-4bb5-94a1-b65b08c3e26f\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"4484bf60-654f-45e9-80cf-b47c05da6073\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"848f9ac9-2b1e-493d-8009-6cb7be269db5\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"ff635ee5-3251-40c4-aca8-630c5c3c22bc\",\"type\":\"LinearScale\"}},\"id\":\"f7eb941a-fec3-40a6-a79a-923a715f8693\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"months\":[0,4,8]},\"id\":\"61196fe2-d9bb-44bc-8430-b0173232b4a7\",\"type\":\"MonthsTicker\"},{\"attributes\":{\"days\":[1,15]},\"id\":\"1c2dec55-41a9-4b08-801e-c668c2483e07\",\"type\":\"DaysTicker\"},{\"attributes\":{\"formatter\":{\"id\":\"67ec2ebd-e7d2-426c-9f80-a384010e6684\",\"type\":\"DatetimeTickFormatter\"},\"plot\":{\"id\":\"f7eb941a-fec3-40a6-a79a-923a715f8693\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"7bb54e78-6a53-43ff-a286-44643943044d\",\"type\":\"DatetimeTicker\"}},\"id\":\"97a165ee-0959-41ce-8803-d300b87d829b\",\"type\":\"DatetimeAxis\"},{\"attributes\":{\"num_minor_ticks\":5,\"tickers\":[{\"id\":\"6740e85a-d96f-4e57-b036-07f76b4bbd66\",\"type\":\"AdaptiveTicker\"},{\"id\":\"05d106bb-3298-4339-be79-345751c81336\",\"type\":\"AdaptiveTicker\"},{\"id\":\"fa427535-6aa3-4e5b-ad37-bc4126a94dc4\",\"type\":\"AdaptiveTicker\"},{\"id\":\"34ee125a-76c2-400e-a2fe-ee82527e3ebf\",\"type\":\"DaysTicker\"},{\"id\":\"a89741a3-f487-48bf-8fcc-45835fc683f7\",\"type\":\"DaysTicker\"},{\"id\":\"d82e2ef6-6b8c-4e5b-bc92-0f8b3c4dfcf6\",\"type\":\"DaysTicker\"},{\"id\":\"1c2dec55-41a9-4b08-801e-c668c2483e07\",\"type\":\"DaysTicker\"},{\"id\":\"d287a14b-55ed-4f53-b100-0d9ee9341369\",\"type\":\"MonthsTicker\"},{\"id\":\"2c87fd05-a4c2-47fd-970d-c88a625ce826\",\"type\":\"MonthsTicker\"},{\"id\":\"61196fe2-d9bb-44bc-8430-b0173232b4a7\",\"type\":\"MonthsTicker\"},{\"id\":\"61f3d5f2-3e9f-43c5-8a55-729afa31df0d\",\"type\":\"MonthsTicker\"},{\"id\":\"e85b75ce-ae6a-4a30-bf9e-92c6454a239a\",\"type\":\"YearsTicker\"}]},\"id\":\"7bb54e78-6a53-43ff-a286-44643943044d\",\"type\":\"DatetimeTicker\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"f7eb941a-fec3-40a6-a79a-923a715f8693\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"ebca3c5f-c409-42c3-bac0-cad93d1a38a8\",\"type\":\"BasicTicker\"}},\"id\":\"a085a0bd-a793-4988-890e-9a8dd0a5b725\",\"type\":\"Grid\"},{\"attributes\":{\"mantissas\":[1,2,5],\"max_interval\":500.0,\"num_minor_ticks\":0},\"id\":\"6740e85a-d96f-4e57-b036-07f76b4bbd66\",\"type\":\"AdaptiveTicker\"},{\"attributes\":{\"months\":[0,2,4,6,8,10]},\"id\":\"2c87fd05-a4c2-47fd-970d-c88a625ce826\",\"type\":\"MonthsTicker\"},{\"attributes\":{},\"id\":\"9dd29f06-050b-473e-92f3-8077d8b11cd4\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"bottom\":{\"value\":0},\"fill_color\":{\"value\":\"red\"},\"left\":{\"field\":\"Object_Entered\"},\"line_color\":{\"value\":\"red\"},\"right\":{\"field\":\"Object_Exited\"},\"top\":{\"value\":1}},\"id\":\"4d49c838-1497-41d8-a5f0-9fd214d0486b\",\"type\":\"Quad\"}],\"root_ids\":[\"f7eb941a-fec3-40a6-a79a-923a715f8693\"]},\"title\":\"Bokeh Application\",\"version\":\"0.13.0\"}};\n",
       "  var render_items = [{\"docid\":\"2f17e818-f8c5-4ea1-984a-bfbcfeb9fb96\",\"notebook_comms_target\":\"7097e455-fcfe-4968-ae1c-6bad82c0af95\",\"roots\":{\"f7eb941a-fec3-40a6-a79a-923a715f8693\":\"2fd45c61-079b-4c7c-be32-1fe268b7a6ef\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\")\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "f7eb941a-fec3-40a6-a79a-923a715f8693"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><code>&lt;Bokeh Notebook handle for <strong>In[3]</strong>&gt;</code></p>"
      ],
      "text/plain": [
       "<bokeh.io.notebook.CommsHandle at 0x1f66839f2b0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bokeh.plotting import figure, show, output_file, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "\n",
    "\n",
    "\n",
    "cds = ColumnDataSource(df)\n",
    "\n",
    "hover = HoverTool(tooltips=[\n",
    "    \n",
    "    ((\"Enter\"), \"(@Start_string)\"),\n",
    "    ((\"Exit\"),\"(@End_string)\")\n",
    "    \n",
    "])\n",
    "\n",
    "plot= figure(x_axis_type = 'datetime', height = 200 , width = 600, title = 'Motion Detection Graph', tools =[hover])\n",
    "plot.yaxis.minor_tick_line_color = None\n",
    "plot.ygrid[0].ticker.desired_num_ticks = 1 \n",
    "\n",
    "\n",
    "\n",
    "#hover=HoverTool(tooltips=[(\"Start\",\"@Start_string\"),(\"End\",\"@End_string\")])\n",
    "plot.add_tools(hover)\n",
    "\n",
    "quad_plot = plot.quad(left= 'Object_Entered', right = 'Object_Exited', bottom = 0, top = 1, color = 'red', source = cds)\n",
    "\n",
    "output_notebook()\n",
    "show(plot,notebook_handle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion: \n",
    "\n",
    "In conclusion: \n",
    "\n",
    "1. We were able to learn OpenCV's basic webcam methods. \n",
    "\n",
    "2. We were able to apply simple motion detection ideas. \n",
    "\n",
    "3. Save the times of entry & exit \n",
    "\n",
    "4. Visualize the times using Bokeh\n",
    "\n",
    "Feel free to play around with the code. \n",
    "\n",
    "I hope that helped you getting started with OpenCV. \n",
    "\n",
    "Good luck! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
